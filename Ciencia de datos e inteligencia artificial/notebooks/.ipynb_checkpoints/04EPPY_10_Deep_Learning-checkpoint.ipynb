{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b33bb718",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/viu_logo.png\" width=\"200\">\n",
    "\n",
    "## 04EPPY - Ciencia de Datos e Inteligencia Artificial\n",
    "### Deep Learning\n",
    "\n",
    "![logo](img/python_logo.png)\n",
    "\n",
    "*Òscar Garibo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b011835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd92f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38425853",
   "metadata": {},
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(8,)))       # capa oculta de 10 nodos, con 8 entradas\n",
    "model.add(Dense(1))                          # capa de salida de 1 nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(8,)))\n",
    "model.add(Dense(80))\n",
    "model.add(Dense(30))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(5))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2da257",
   "metadata": {},
   "source": [
    "## Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31160f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the functional api\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the layers\n",
    "x_in = Input(shape=(8,))\n",
    "x = Dense(100)(x_in)\n",
    "x = Dense(80)(x)\n",
    "x = Dense(30)(x)\n",
    "x = Dense(10)(x)\n",
    "x = Dense(5)(x)\n",
    "x_out = Dense(1)(x)\n",
    "# define the model\n",
    "model = Model(inputs=x_in, outputs=x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77696a7",
   "metadata": {},
   "source": [
    "## MLP para Clasificación Binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc928335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for binary classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from numpy import set_printoptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94767b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a96dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1)\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {acc:.3f}')\n",
    "# make a prediction\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "y_pred = model.predict([row])\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc9913",
   "metadata": {},
   "source": [
    "## MLP para Clasificación Multiclase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for multiclass classification\n",
    "from numpy import set_printoptions\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec38095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {acc:.3f}')\n",
    "# make a prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "y_pred = model.predict([row])\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: {y_pred} (class={argmax(y_pred)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ed682",
   "metadata": {},
   "source": [
    "## MLP para Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1363d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for regression\n",
    "from numpy import set_printoptions\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70946f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(path, header=None)\n",
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1473bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1)\n",
    "# evaluate the model\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'MSE: {error:.3f}, RMSE: {sqrt(error):.3f}')\n",
    "# make a prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "y_pred = model.predict([row])\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a073e",
   "metadata": {},
   "source": [
    "## CNN Redes Convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading and plotting the mnist dataset\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "# summarize loaded dataset\n",
    "print(f'Train: X={trainX.shape}, y={trainy.shape}')\n",
    "print(f'Test: X={testX.shape}, y={testy.shape}')\n",
    "# plot first few images\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "\t# plot raw pixel data\n",
    "\tax[i].imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb44da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a cnn for image classification\n",
    "from numpy import set_printoptions\n",
    "from numpy import asarray\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "# reshape data to have a single channel\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
    "# determine the shape of the input images\n",
    "in_shape = x_train.shape[1:]\n",
    "# determine the number of classes\n",
    "n_classes = len(unique(y_train))\n",
    "print(in_shape, n_classes)\n",
    "# normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "# define loss and optimizer\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Accuracy: {acc:.3f}')\n",
    "# make a prediction\n",
    "image = x_train[0]\n",
    "y_pred = model.predict(asarray([image]))\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: class={argmax(y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[15]\n",
    "y_pred = model.predict(asarray([image]))\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: class={argmax(y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5406b",
   "metadata": {},
   "source": [
    "## RNN Redes Recurrentes, Long Short-Term Memory LSTM y GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0890b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm for time series forecasting\n",
    "from numpy import set_printoptions\n",
    "from numpy import sqrt\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        #gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return asarray(X), asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
    "# retrieve the values\n",
    "values = df.values.astype('float32')\n",
    "# specify the window size\n",
    "n_steps = 5\n",
    "# split into samples\n",
    "X, y = split_sequence(values, n_steps)\n",
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5224e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "#model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'MSE: {mse:.3f}, RMSE: {sqrt(mse):.3f}, MAE: {mae:.3f}')\n",
    "# make a prediction\n",
    "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "y_pred = model.predict(row)\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cffef55",
   "metadata": {},
   "source": [
    "## Visualize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of summarizing a model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8328185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', kernel_initializer='he_normal', input_shape=(8,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal', name='capa1'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7296d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of plotting a model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59448650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(8,), name='conv1'))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal', name='Dense1'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# summarize the model\n",
    "plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc4c79",
   "metadata": {},
   "source": [
    "## Visualizar Curvas de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f1898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of plotting learning curves\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315586b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbe306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
    "epochs = 300\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# fit the model\n",
    "history = model.fit(X, y, epochs=epochs, batch_size=32, verbose=0, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "epoch_values = list(range(epochs))\n",
    "ax.plot(epoch_values, history.history['loss'], label='Pérdida de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_loss'], label='Pérdida de validación')\n",
    "ax.plot(epoch_values, history.history['accuracy'], label='Exactitud de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_accuracy'], label='Exactitud de validación')\n",
    "\n",
    "ax.set_title('Pérdida y Exactitud de Entrenamiento')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Pérdida/Exactitud')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05d60d",
   "metadata": {},
   "source": [
    "## Guardar y cargar un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31858a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of saving a fit model\n",
    "from numpy import set_printoptions\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "sgd = SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy')\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=0, validation_split=0.3)\n",
    "# save model to file\n",
    "model.save('res/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from file\n",
    "model2 = load_model('res/model.h5')\n",
    "# make a prediction\n",
    "row = [1.91518414, 1.14995454, -1.52847073, 0.79430654]\n",
    "y_pred = model2.predict([row])\n",
    "set_printoptions(precision=3)\n",
    "print(f'Predicted: {y_pred[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4c978",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using dropout\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe06089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc286178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67caa0cd",
   "metadata": {},
   "source": [
    "## Normalización de Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using batch normalization\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18513202",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24937607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using early stopping\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21513b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# configure early stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# fit the model\n",
    "history = model.fit(X, y, epochs=200, batch_size=32, verbose=1, validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfe50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "epoch_values = list(range(len(history.history['loss'])))\n",
    "ax.plot(epoch_values, history.history['loss'], label='Pérdida de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_loss'], label='Pérdida de validación')\n",
    "\n",
    "ax.set_title('Pérdida y Exactitud de Entrenamiento')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Pérdida/Exactitud')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "epoch_values = list(range(len(history.history['loss'])))\n",
    "ax.plot(epoch_values, history.history['loss'], label='Pérdida de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_loss'], label='Pérdida de validación')\n",
    "\n",
    "ax.set_title('Pérdida y Exactitud de Entrenamiento')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Pérdida/Exactitud')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f99b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "epoch_values = list(range(len(history.history['loss'])))\n",
    "ax.plot(epoch_values, history.history['loss'], label='Pérdida de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_loss'], label='Pérdida de validación')\n",
    "\n",
    "ax.set_title('Pérdida y Exactitud de Entrenamiento')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Pérdida/Exactitud')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "epoch_values = list(range(len(history.history['loss'])))\n",
    "ax.plot(epoch_values, history.history['loss'], label='Pérdida de entrenamiento')\n",
    "ax.plot(epoch_values, history.history['val_loss'], label='Pérdida de validación')\n",
    "\n",
    "ax.set_title('Pérdida y Exactitud de Entrenamiento')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Pérdida/Exactitud')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "fig.savefig('here.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69b1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
